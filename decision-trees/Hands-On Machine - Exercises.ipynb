{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c96198",
   "metadata": {},
   "source": [
    "# Hands-On Machine Learning with Scikit-Learn and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48172ad0",
   "metadata": {},
   "source": [
    "## Chapter 6 - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30735f",
   "metadata": {},
   "source": [
    "#### 1. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 1 million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff935b4",
   "metadata": {},
   "source": [
    "> A Decision Tree without restrictions will grow until each leaf node has a single sample. \n",
    "Let N be the total number of samples, the approximate depth will be the log2(N). Thus we obtain ~20 levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd10a367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.931568569324174"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log2(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c4f3d",
   "metadata": {},
   "source": [
    "#### 2. Is a node’s Gini impurity generally lower or greater than its parent’s? Is it generally lower/greater, or always lower/greater?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc598a9d",
   "metadata": {},
   "source": [
    "> Due to the nature of the CART algorithm, the split will look for minimizing the total Gini impurity. This can be achieved by either both branches having a lower impurity or one of them outsetting the other by having a way lower impurity. Thus generally the nodes have a lower impurity compared to its parent's, however each node individually may have a greater impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6de9e",
   "metadata": {},
   "source": [
    "#### 3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6bbd43",
   "metadata": {},
   "source": [
    "> Yes, since by reducing the max_depth, you restrict how many decisions nodes can be created, thus limiting the learning capacity of the model and avoiding overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6651742",
   "metadata": {},
   "source": [
    "#### 4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40735d",
   "metadata": {},
   "source": [
    "> No, since the CART algorithm doesn't employ the nominal value of each feature and only its ranking capacity, scaling the features won't have any effect. However this only applies to monotonic transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299306ad",
   "metadata": {},
   "source": [
    "#### 5. If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659ef7a",
   "metadata": {},
   "source": [
    "> The complexity of a training a Decision Tree is O(n * m(log(m))), therefore by dividing the complexity of each example we get: 10M * log(10M) / 1M * log(1M) ~ 11.7. So by multiplying the number of samples by 10, we multiply the training time by 11.7, therefore it would take roughly 11.7 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f41a7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.666666666666666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10E6 * np.log2(10E6) / (1E6 * np.log2(1E6)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7232d381",
   "metadata": {},
   "source": [
    "#### 6. If your training set contains 100,000 instances, will setting presort=True speed up training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dac022",
   "metadata": {},
   "source": [
    "> Probably not, the presort may bring benefits in terms of finding the best decision nodes faster, however the sorting action necessary for this much data may actually slow down the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acadb38",
   "metadata": {},
   "source": [
    "#### 7. Train and fine-tune a Decision Tree for the moons dataset.\n",
    "- a. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4)\n",
    "- b. Split it into a training set and a test set using train_test_split()\n",
    "- c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes.\n",
    "- d. Train it on the full training set using these hyperparameters, and measure your model’s performance on the test set. You should get roughly 85% to 87% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d870da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44192ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_moons(n_samples=10000, noise=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f45f9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset[0], dataset[1], test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "004e5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "177b4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [1, 2, 4, 6, 8, 10, 12, 14, 16, 18],\n",
    "    'min_samples_split': [1, 2, 4, 6, 8, 10, 12, 14, 16, 18], \n",
    "    'max_features': [\"auto\", \"sqrt\", \"log2\"],\n",
    "    'max_leaf_nodes': [1, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdf5f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(clf, params, cv= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06c5fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vgora\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "1710 fits failed out of a total of 9000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vgora\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\vgora\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\vgora\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "810 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vgora\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\vgora\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\vgora\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 314, in fit\n",
      "    raise ValueError(\n",
      "ValueError: max_leaf_nodes 1 must be either None or larger than 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\vgora\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.84597081 0.85149389 0.84880558]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'max_depth': [1, 2, 4, 6, 8, 10, 12, 14, 16, 18],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'max_leaf_nodes': [1, 2, 4, 6, 8, 10, 12, 14, 16, 18],\n",
       "                         'min_samples_split': [1, 2, 4, 6, 8, 10, 12, 14, 16,\n",
       "                                               18]})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c838da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40f681bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85af660d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8442424242424242"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c663191",
   "metadata": {},
   "source": [
    "#### 8. Grow a forest.\n",
    "- a. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use ScikitLearn’s ShuffleSplit class for this.\n",
    "- b. Train one Decision Tree on each subset, using the best hyperparameter values found above. Evaluate these 1,000 Decision Trees on the test set. Since they were trained on smaller sets, these Decision Trees will likely perform worse than the first Decision Tree, achieving only about 80% accuracy.\n",
    "- c. Now comes the magic. For each test set instance, generate the predictions of the 1,000 Decision Trees, and keep only the most frequent prediction (you can use SciPy’s mode() function for this). This gives you majority-vote predictions over the test set.\n",
    "- d. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a Random Forest classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1454521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba45a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ShuffleSplit(n_splits = 1000, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ec522aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_list = []\n",
    "acc_scores = []\n",
    "predictions = []\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(rs.split(X_train)):\n",
    "    weak_learner = DecisionTreeClassifier(\n",
    "        max_depth =  2,\n",
    "        min_samples_split = 12, \n",
    "        max_features = \"sqrt\",\n",
    "        max_leaf_nodes = 8\n",
    "    )\n",
    "    weak_learner.fit(X_train[train_index], y_train[train_index])\n",
    "    decision_tree_list.append(weak_learner)\n",
    "    \n",
    "    y_pred = weak_learner.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "    \n",
    "    acc_score = accuracy_score(y_pred, y_test)\n",
    "    acc_scores.append(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c803541",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score = sum(acc_scores)/len(acc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b4e04c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy score of individual weak learner is 0.7825415151515164\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average accuracy score of individual weak learner is {average_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f268dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "predictions_array= np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b3d0772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3300"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mode(predictions_array)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "601783a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_rf = accuracy_score(mode(predictions_array)[0][0], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "764686a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy score of joined weak learner is 0.8606060606060606\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average accuracy score of joined weak learner is {accuracy_score_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88c50f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09975770479017876"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_rf/average_score - 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
